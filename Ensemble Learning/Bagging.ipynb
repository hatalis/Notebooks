{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tutorial: Bagging**\n",
    "### By Kostas Hatalis\n",
    "\n",
    "**Prerequisite Notebooks:** *Decision Trees, Ensemble Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Bagging, also known as bootstrap aggregation, introduced in 1996 by Leo Breiman [1] is an ensemble method involving training the same algorithm many times using different subsets sampled with replacement (known as boostrap sampling) from the training data. It then aggregates their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to **reduce the variance** of a base estimator (e.g., a CART or ANN), by introducing randomization into its construction procedure and then making an ensemble out of it.\n",
    "\n",
    "*As they provide a way to reduce overfitting, **bagging methods work best with strong and complex models** (e.g., fully developed decision trees), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees).*\n",
    "\n",
    "*When bagging with decision trees, we are less concerned about individual trees overfitting the training data. For this reason and for efficiency, the individual decision trees are grown deep (e.g. few training samples at each leaf-node of the tree) and the trees are not pruned. These trees will have both high variance and low bias. These are important characterize of sub-models when combining predictions using bagging. The only parameters when bagging decision trees is the number of samples and hence the number of trees to include. This can be chosen by increasing the number of trees on run after run until the accuracy begins to stop showing improvement.*\n",
    "\n",
    "Bagging methods come in many flavours but mostly differ from each other by the way they draw random subsets of the training set:\n",
    "- **Bagging**: samples are drawn with replacement.\n",
    "- **Pasting**: samples are drawn without replacement (need big training set).\n",
    "- **Random Subspaces**: random subsets of the dataset are drawn as random subsets of the features.\n",
    "- **Random Patches**: when base estimators are built on subsets of both samples and features.\n",
    "\n",
    "**Advantages:**\n",
    "- Reduces over-fitting of the model.\n",
    "- Handles higher dimensionality data very well.\n",
    "- Maintains accuracy for missing data.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Since final prediction is based on the mean predictions from subset trees, it won’t give precise values for the classification and regression model.\n",
    "\n",
    "**Random Forest**: is an algorithm (explained in another Notebook) that makes a small tweak to Bagging and results in a very powerful prediction method.\n",
    "\n",
    "Below the process of bootstrapping and bagging is illustrated (from [2]):\n",
    "\n",
    "<img src=\"images/bagging.PNG\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92 (+/- 0.02) [Decision Tree]\n",
      "Accuracy: 0.96 (+/- 0.03) [Bagging]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets  import load_breast_cancer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# load data\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Instantiate a classification-tree 'dt'\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Instantiate a BaggingClassifier 'bc'\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=300)\n",
    "\n",
    "# Get accuracy score using CV for dt\n",
    "scores = cross_val_score(dt, X, y, cv=5, scoring='accuracy')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), 'Decision Tree'))\n",
    "\n",
    "# Get accuracy score using CV for bc\n",
    "scores = cross_val_score(bc, X, y, cv=5, scoring='accuracy')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), 'Bagging'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## **Which is the best:** Bagging or Boosting?\n",
    "\n",
    "There’s not an outright winner; it depends on the data, the simulation and the circumstances.\n",
    "Bagging and Boosting decrease the variance of your single estimate as they combine several estimates from different models. So the result may be a model with higher stability.\n",
    "\n",
    "If the problem is that the single model gets a very low performance, Bagging will rarely get a better bias. However, Boosting could generate a combined model with lower errors as it optimises the advantages and reduces pitfalls of the single model.\n",
    "\n",
    "By contrast, if the difficulty of the single model is over-fitting, then Bagging is the best option. Boosting for its part doesn’t help to avoid over-fitting; in fact, this technique is faced with this problem itself. For this reason, Bagging is effective more often than Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## **References**\n",
    "\n",
    "[1] Breiman, Leo. \"Bagging predictors.\" Machine learning 24.2 (1996): 123-140.\n",
    "\n",
    "[2] https://campus.datacamp.com/courses/machine-learning-with-tree-based-models-in-python/\n",
    "\n",
    "[3] https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/\n",
    "\n",
    "[4] https://scikit-learn.org/stable/modules/ensemble.html#bagging-meta-estimator\n",
    "\n",
    "[5] https://en.wikipedia.org/wiki/Bootstrap_aggregating"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
