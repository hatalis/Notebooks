{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tutorial: Stacking**\n",
    "### By Kostas Hatalis\n",
    "\n",
    "**Prerequisite Notebooks:** *Decision Trees, Ensemble Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## **Stacking**\n",
    "\n",
    "Stacking (also called stacked generalization or super learning), introduced in 1992 by Leo Breiman [1], involves training a **meta-model** on **meta-features** which are the predictions of several base learning algorithms with the aim of reducing the generalization error. In other words, the basic idea is to train several base models, and feed their predictions into a another model that learns to weigh and add the base predictions to get (ideally) better predictions.\n",
    "\n",
    "For classification, the The meta-classifier can either be trained on the predicted class labels or probabilities from the ensemble. Stacking can also be used for regression known as stacked regression, introduced in 1996 [2]. The meta-regressor uses the numeric predictions from the individual base regressors as inputs to make a final prediction.\n",
    "\n",
    "The standard stacking procedure, is to fit the base (first-level) models to the whole training set. Then use their predictions and the whole training set again to prepare the inputs for the meta (second-level) model. This type of Stacking is prone to **overfitting due to information leakage, and should be avoided.** Thus, it is advised to use stacking with the Cross-Validation (CV) algorithm.\n",
    "\n",
    "For classification and regression, CV based stacking works as follows:\n",
    "1. Split data set into training and testing sets.\n",
    "2. Take the training set and split into k folds.\n",
    "    - $k-1$ folds are used for training and 1 fold used for validation.\n",
    "3. Fit the base models on the $k-1$ training folds.\n",
    "4. Apply the base learners to predict the validation fold.\n",
    "5. Stack the resulting predictions as input data to the meta-model.\n",
    "6. Repeat steps 2 to 5 until the whole training set has been cycled through to create a full stack of predictions as input to the meta-model.\n",
    "7. Train the meta-model on the stacked predictions.\n",
    "8. After the meta-model has been trained, retrain the base models on the entire training set.\n",
    "    - At this point validate your model on the testing set.\n",
    "\n",
    "This process is illustrated  in the figure below (from [3]) for classification and regression:\n",
    "\n",
    "<img src=\"images/stacking.PNG\" width=\"800\">\n",
    "\n",
    "**Stacking with CV typically yields performance better than any single one of the trained base models.** \n",
    "\n",
    "### **Diversity**\n",
    "\n",
    "It is important to try a diverse type of base and meta models!\n",
    "\n",
    "In practice, a logistic regression model is often used as the meta-model. However any algorithm could be used as the meta-model. Stacking with nonlinear meta-models, such as GBMs and ANNs, for multiclass problems gives surprising gains. \n",
    "\n",
    "In the base models, the same algorithm could also be used multiple times with different training algorithms, different hyperparameters, and different feature subsets. For instance, you could have 20 neural networks, 20 support vector machines, and 20 random forests as the base models. There's no limit on how many models you can use, but after some point you will reach a plateau of performance after a certain number of models.\n",
    "\n",
    "### **Blending**\n",
    "\n",
    "The top-performers in the 2006 Netflix competition introduced a form of stacking called blending. With blending, instead of creating out-of-fold predictions for the train set, you create a small holdout set of say 10% of the train set. The stacker model then trains on this holdout set only. It is simpler than stacking and it wards against an information leak where stackers use different data. However, you use less data overall and the final model may overfit to the holdout set, whereas stacking is more robust with CV. As for performance, both techniques are able to give similar results.\n",
    "\n",
    "### **Multi-Layered Stacking**\n",
    "\n",
    "Stacking is not restricted to just two layers, in theory you can add as many layers as you like. One layer feeds its predictions as features into the next layer of models. K-fold CV is again applied to each layer with the data avaliable from the layer below. While not as common as 2-layer stacking or other ensemble methods, due to complexity issues, multi-layered stacking can be fairly powerful and has been used as the winning approach of several Kaggle and KDD Cup competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Code Tutorial CV Stacking**\n",
    "\n",
    "Sklearn has no support for stacking. But there is another library that does called **mlxtend** [3] which extends sklearn.\n",
    "\n",
    "mlxtend implements CV stacking with `StackingCVClassifier` and `StackingCVRegressor`.\n",
    "\n",
    "Both these functions also support grid search and training base models on subsets of features [4].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-fold cross validation:\n",
      "\n",
      "Accuracy: 0.91 (+/- 0.01) [KNN]\n",
      "Accuracy: 0.91 (+/- 0.06) [Random Forest]\n",
      "Accuracy: 0.92 (+/- 0.03) [Naive Bayes]\n",
      "Accuracy: 0.95 (+/- 0.04) [Stacking]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for all methods\n",
    "RANDOM_SEED = 1\n",
    "\n",
    "# Load in dataset\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "# Instantiate classifiers\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# The StackingCVClassifier uses scikit-learn's check_cv\n",
    "# internally, which doesn't support a random seed. Thus\n",
    "# NumPy's random seed need to be specified explicitely for\n",
    "# deterministic behavior\n",
    "np.random.seed(RANDOM_SEED)\n",
    "sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3], \n",
    "                            use_probas=True,\n",
    "                            meta_classifier=lr,\n",
    "                            cv=5)\n",
    "\n",
    "print('3-fold cross validation:\\n')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, sclf], \n",
    "                      ['KNN', \n",
    "                       'Random Forest', \n",
    "                       'Naive Bayes',\n",
    "                       'Stacking']):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Code Tutorial Multi-Layered Stacking**\n",
    "\n",
    "One of the few (clean) libraries I found that implements multi-layer stacking is **ml-ensemble** [5].\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make multi-level stacking example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## **References**\n",
    "\n",
    "[1] Wolpert, D., Stacked Generalization., Neural Networks, 5(2), pp. 241-259., 1992\n",
    "\n",
    "[2] Breiman, Leo. \"Stacked regressions.\" Machine learning 24.1 (1996): 49-64.\n",
    "\n",
    "[3] https://rasbt.github.io/mlxtend/\n",
    "\n",
    "[4] https://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/#methods\n",
    "    \n",
    "[5] http://ml-ensemble.com/info/start/ensembles.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
